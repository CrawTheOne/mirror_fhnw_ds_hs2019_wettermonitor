{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Datenanalyse und Wahrscheinlichkeitsrechnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "import pandas as pd\n",
    "from influxdb import DataFrameClient, InfluxDBClient\n",
    "import pytz\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DataFrameClient(host = config.DB_HOST, port = config.DB_PORT, database = config.DB_DBNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowindex_as_col(df):\n",
    "    \"\"\"add row index(time) to new column. df = dataframe, name_col = new column name\"\"\"\n",
    "    df.index.name = \"Time\"\n",
    "    df = df.reset_index(inplace=False)\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "def appropriate_dtypes(df, column_name, result_dtype ):\n",
    "    \"\"\"Datentypen ändern\"\"\"\n",
    "    df = df.astype({column_name: result_dtype})\n",
    "    return df\n",
    "    \n",
    "def select_timedelta(time_offset_days,  time_delta_in_days):\n",
    "    \"\"\"Make a Select Statement on pass over to new df a certain timedelta from NOW / double_output!: Output1, Output2 = func() /\n",
    "    example: time_delta_in_days = 10\n",
    "    Inputs: time_offset_days = 0, time_delta_in_days = 365\n",
    "    \"\"\"\n",
    "    #Set time relative to now for Query (today: 00:00:00)\n",
    "    now = datetime.datetime.today()\n",
    "    start = now - datetime.timedelta(days = time_offset_days)\n",
    "    past = now - datetime.timedelta(days = time_delta_in_days)\n",
    "\n",
    "    #Set start and end time\n",
    "    end_time = start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    start_time = past.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # NoSQL Query  (to be added: timezone adjusting)\n",
    "    query = \"SELECT * FROM \\\"{}\\\",\\\"{}\\\" WHERE time >= '{}' AND time <= '{}' \"\\\n",
    "                        .format(config.stations[0], config.stations[1], start_time, end_time)\n",
    "    df_temp = client.query(query)\n",
    "\n",
    "    # to create pandas df, use only one dicitonary part (mythenquai, tiefenbrunnen)\n",
    "    df_mythenquai = pd.DataFrame(df_temp['mythenquai'])\n",
    "    df_tiefenbrunnen = pd.DataFrame(df_temp['tiefenbrunnen'])\n",
    "    \n",
    "    return df_mythenquai, df_tiefenbrunnen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /query?q=SELECT+%2A+FROM+%22mythenquai%22%2C%22tiefenbrunnen%22+WHERE+time+%3E%3D+%272019-12-20+23%3A41%3A06%27+AND+time+%3C%3D+%272019-12-30+23%3A41%3A06%27+&db=meteorology (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000262D6D2ED48>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[1;32m--> 159\u001b[1;33m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0msock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1243\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1289\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1290\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1238\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    965\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m             raise NewConnectionError(\n\u001b[1;32m--> 168\u001b[1;33m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x00000262D6D2ED48>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[1;32m--> 638\u001b[1;33m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[0;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /query?q=SELECT+%2A+FROM+%22mythenquai%22%2C%22tiefenbrunnen%22+WHERE+time+%3E%3D+%272019-12-20+23%3A41%3A06%27+AND+time+%3C%3D+%272019-12-30+23%3A41%3A06%27+&db=meteorology (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000262D6D2ED48>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-9db16f7c3338>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf2\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mselect_timedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-71-e259150a5e9b>\u001b[0m in \u001b[0;36mselect_timedelta\u001b[1;34m(time_offset_days, time_delta_in_days)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"SELECT * FROM \\\"{}\\\",\\\"{}\\\" WHERE time >= '{}' AND time <= '{}' \"\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                         \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdf_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# to create pandas df, use only one dicitonary part (mythenquai, tiefenbrunnen)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\influxdb\\_dataframe_client.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, query, params, bind_params, epoch, expected_response_code, database, raise_errors, chunked, chunk_size, method, dropna)\u001b[0m\n\u001b[0;32m    192\u001b[0m                           \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                           chunk_size=chunk_size)\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrameClient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mquery_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SELECT\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\influxdb\\client.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, query, params, bind_params, epoch, expected_response_code, database, raise_errors, chunked, chunk_size, method)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             \u001b[0mexpected_response_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_response_code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m         )\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\influxdb\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, url, method, params, data, expected_response_code, headers)\u001b[0m\n\u001b[0;32m    281\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                     \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_verify_ssl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m                 )\n\u001b[0;32m    285\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=8086): Max retries exceeded with url: /query?q=SELECT+%2A+FROM+%22mythenquai%22%2C%22tiefenbrunnen%22+WHERE+time+%3E%3D+%272019-12-20+23%3A41%3A06%27+AND+time+%3C%3D+%272019-12-30+23%3A41%3A06%27+&db=meteorology (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000262D6D2ED48>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte'))"
     ]
    }
   ],
   "source": [
    "df1, df2  = select_timedelta(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Data from InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mythenquai_1y, tiefenbrunnen_1y = select_timedelta(0, 365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiefenbrunnen_1y = appropriate_dtypes(tiefenbrunnen_1y, \"humidity\" , np.int8)\n",
    "tiefenbrunnen_1y = appropriate_dtypes(tiefenbrunnen_1y, \"wind_direction\", np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mythenquai_1y = appropriate_dtypes(mythenquai_1y, \"humidity\", np.int8)\n",
    "mythenquai_1y = appropriate_dtypes(mythenquai_1y,\"wind_direction\", np.int16 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aesthetische Einstellungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(tiefenbrunnen_1y.describe(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(mythenquai_1y.describe(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korrelationen Bereich Temperatur und Feuchte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiefenbrunnen_adapt_temp = tiefenbrunnen_1y.drop( columns = [\"wind_direction\",\"wind_force_avg_10min\",\n",
    "                                                             \"wind_gust_max_10min\",\"wind_speed_avg_10min\",\n",
    "                                                             \"windchill\", \"barometric_pressure_qfe\"])\n",
    "\n",
    "tiefenbrunnen_adapt_temp = rowindex_as_col(tiefenbrunnen_adapt_temp)\n",
    "tiefenbrunnen_adapt_temp.head()\n",
    "\n",
    "sns.pairplot(tiefenbrunnen_adapt_temp, kind = \"reg\", diag_kind = \"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auffälligkeiten:\n",
    "- Luftfeuchtigkeit sollte mit Temperatur steigen. Erklärung?\n",
    "- Eigentlich sollte der Taupunkt bei 100% relativer Luftfeuchte sein. Erklärung?\n",
    "- Es bestehen positive Korrelationen zwischen Lufttemperatur, Wassertemperatur und Taupunkt.\n",
    "- Negative Korrelationen zwischen Lufttemperatur und Luftfeuchtigkeit.\n",
    "\n",
    "#### Bemerkungen\n",
    "- Barometrischer Druck entfernt, da schwache Korrelationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korrelationen Bereich Wind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiefenbrunnen_adapt_wind = tiefenbrunnen_1y.drop( columns = [\"air_temperature\",\"barometric_pressure_qfe\",\"dew_point\",\n",
    "                                                             \"humidity\", \"water_temperature\", \"windchill\"])\n",
    "\n",
    "sns.pairplot(tiefenbrunnen_adapt_wind, kind = \"reg\", diag_kind = \"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auffällligkeiten:\n",
    "- Positive Korrelation zwischen Windrichtung, Geschwindigkeiten und WIndstärke\n",
    "- Starke Positive Korrelation zwischen Windgeschwindigkeit, Stärke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zeitliche Datenanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tiefenbrunnen_1y_time = rowindex_as_col(tiefenbrunnen_1y)\n",
    "\n",
    "sns.lineplot(x= tiefenbrunnen_1y_time[\"Time\"], y= tiefenbrunnen_1y_time[\"air_temperature\"])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einfache Voraussage anhand Verteilung der Temperaturen oder des Windes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mythenquai_3d, tiefenbrunnen_3d = select_timedelta(0, 3) # Get data with function for 3days\n",
    "mythenquai_3d, tiefenbrunnen_3d = rowindex_as_col(mythenquai_3d), rowindex_as_col(tiefenbrunnen_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mythenquai_3d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x = mythenquai_3d[\"Time\"], y = mythenquai_3d[\"air_temperature\"]  ,data = mythenquai_3d )\n",
    "plt.title(\"Temperatur letzte 3 Tage\")\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorhersage der Temperatur anhand der Temperaturverteilung der letzten 3 Tage. Beispiel: Wahrscheinlichkeit fast 40 %, dass Temperatur 4° wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(mythenquai_3d[\"air_temperature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(mythenquai_3d[\"wind_speed_avg_10min\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prognose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Referenz werden die Temperaturdaten aus dem Jahr 2018 genommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = pd.read_csv(\"./influxdb-1.7.8-1/data/messwerte_mythenquai_2007-2018.csv\", index_col=0)\n",
    "#df_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_prediction\n",
    "df_pred.index = pd.to_datetime(df_pred.index)\n",
    "\n",
    "df_pred = df_pred.loc[\"2018-01-01\":\"2018-12-31\"]\n",
    "\n",
    "len(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mögliche Verfahren:\n",
    "    - Messung zur Performance eine Funktion schreiben\n",
    "    - Verfahren KS-Test implementieren\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_in_grouped_days(df, column, group_string, group_int):\n",
    "    \"\"\" \n",
    "    Splits distributions in separate lists of days together as new list of values. This DF is always used as reference to determine temperature of next day.\n",
    "    Input: df = vector or dataframe, column = specific column index, group_string = \"3D\", \"D\", group_int = integer days want to group\n",
    "    \"\"\" \n",
    "    \n",
    "    df.index = pd.to_datetime(df.index) ## Index conversion to datetime\n",
    "\n",
    "    df1 = df.iloc[:, column]\n",
    "    \n",
    "    grouped_df = df1.resample(group_string).aggregate(lambda tdf: tdf.tolist()) #Creates new df by grouping days\n",
    "    grouped_df = pd.DataFrame(grouped_df)\n",
    "    \n",
    "    df2 = df.iloc[:, column]\n",
    "    \n",
    "    grouped_df_max = df2.resample(\"D\").aggregate(lambda tdf: tdf.max())\n",
    "    grouped_df_max = pd.DataFrame(grouped_df_max)\n",
    "    grouped_df_max = grouped_df_max[::group_int].iloc[1:] ## takes each third row and drops the first one\n",
    "\n",
    "    new_df = pd.concat([grouped_df, grouped_df_max], axis = 1)\n",
    "    new_df = new_df.shift(-1).dropna() \n",
    "    new_df.columns = [\"grouped_values\", \"Temp_next_day\"]\n",
    "    \n",
    "    return new_df\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = get_values_in_grouped_days(df_pred, 0, \"1D\", 1)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_max = df_prediction.loc[\"2010-08-02\":\"2010-08-02\"].max()\n",
    "\n",
    "df_test = df_prediction.loc[\"2010-01-01\":\"2018-12-31\"]\n",
    "df_test2 = df_prediction.loc[\"2018-01-11\"]\n",
    "\n",
    "df_test = df_test.iloc[:,0]\n",
    "df_test = pd.DataFrame(df_test)\n",
    "\n",
    "#df_test = df_test.reset_index(drop = True)\n",
    "#df_test.head()\n",
    "#df_test2\n",
    "#df_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_willcoxon(df_for_test, sample_df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    p_val_to_compare = 0\n",
    "    fitting_temp_next_day = 0\n",
    "    iterations = 0 ## length not equal of sample df \n",
    "        \n",
    "    for row in range(0, len(sample_df)):\n",
    "        \n",
    "        if len(sample_df.iloc[row, 0]) == len(df_for_test.iloc[:, 0]):\n",
    "\n",
    "            stat, p_val = spstats.wilcoxon(df_for_test.iloc[:, 0], sample_df.iloc[row, 0]) \n",
    "          \n",
    "            if p_val > p_val_to_compare:\n",
    "                p_val_to_compare = p_val\n",
    "                fitting_temp_next_day = sample_df.iloc[row, 1]\n",
    "         \n",
    "        elif len(sample_df.iloc[row, 0]) > len(df_for_test.iloc[:, 0]): ## Size adjustment if sample df larger than df for test\n",
    "            \n",
    "            sample_df_unequal = sample_df.iloc[row, 0][0:len(df_for_test)]\n",
    "            stat, p_val = spstats.wilcoxon(df_for_test.iloc[:, 0], sample_df_unequal)\n",
    "\n",
    "            if p_val > p_val_to_compare:\n",
    "                p_val_to_compare = p_val\n",
    "                fitting_temp_next_day = sample_df.iloc[row, 1]\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            df_for_test_unequal = df_for_test.iloc[:, 0][0:len(sample_df)] ## size adjustment if df for the test larger than sample df\n",
    "            stat, p_val = spstats.wilcoxon(df_for_test_unequal, sample_df.iloc[row, 0]) \n",
    "\n",
    "            if p_val > p_val_to_compare:\n",
    "                p_val_to_compare = p_val\n",
    "                fitting_temp_next_day = sample_df.iloc[row, 1]\n",
    "\n",
    "        iterations = iterations + 1        \n",
    "            \n",
    "    print(\"P-Wert: {} / Iterations: {} / Fitting Temp.: {}\".format(p_val_to_compare, iterations, fitting_temp_next_day))                                                                                                                            \n",
    "    \n",
    "    return fitting_temp_next_day\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_willcoxon(df_test2, sample_df)\n",
    "\n",
    "for i in range(300):\n",
    "    stat, p_val = spstats.wilcoxon(df_test2.iloc[:, 0], sample_df.iloc[i, 0]) \n",
    "    print(p_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorhersage mit KS-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats as spstats\n",
    "\n",
    "def prediction_ks_test(df_for_test, sample_df):\n",
    "    \"\"\"Predicts the value of next day by using the statistical KS-Test of Scipy package. It's used to compare the distributions of two\n",
    "    test samples. The higher the probability value the better is the fit.\n",
    "    Input: df_for_test: dataframe of one day to perform the test / sample_df: specific sample df as reference\n",
    "    Returns: fitting maximum temperature of next day\n",
    "    \"\"\"\n",
    "    KS_p_val_to_compare = 0\n",
    "    fitting_temp_next_day = 0\n",
    "    iterations = 0 ## length not equal of sample df \n",
    "        \n",
    "    for row in range(0, len(sample_df)):\n",
    "                                \n",
    "        KS_stat, KS_p_val = spstats.ks_2samp(df_for_test.iloc[:, 0], sample_df.iloc[row, 0]) # Perform t-test\n",
    "          \n",
    "        if KS_p_val > KS_p_val_to_compare:\n",
    "            KS_p_val_to_compare = KS_p_val\n",
    "            fitting_temp_next_day = sample_df.iloc[row, 1]\n",
    "            \n",
    "        iterations = iterations + 1        \n",
    "            \n",
    "    #print(\"P-Wert: {} / Iterations: {} / Fitting Temp.: {}\".format(KS_p_val_to_compare, iterations, fitting_temp_next_day))                                                                                                                            \n",
    "    \n",
    "    return fitting_temp_next_day\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Kolmogorov-Smirnoff:\n",
    "Der Durchschnitt aller Messwerte aus einem Jahr.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_test_KS(df_test, sample_df, days_used_for_prediction):\n",
    "    \"\"\"Purpose is to test a generated prediction method of one year. Calculates the average deviation of one simulative test year\n",
    "    Parameters: df_test = df which should be tested(first column) / days_used_... = 1d (how much days're used for the prediction)\n",
    "    \"\"\"\n",
    "    days_offset = days_used_for_prediction         \n",
    "    start_date_str = \"2017-01-01\" ## Check all Values for 2018\n",
    "    start_date = datetime.datetime.strptime(start_date_str,  \"%Y-%m-%d\")\n",
    "    days_in_df_test = len(df_test.index.dayofyear.unique())\n",
    "    \n",
    "    test_results = np.array([])\n",
    "    \n",
    "    for day in range(days_used_for_prediction, days_in_df_test):\n",
    "        ## Maximum Temperature of +1day or how much \n",
    "        time_delta1,time_delta2 = datetime.timedelta(days = day), datetime.timedelta(days = day + 1)\n",
    "        val_date1, val_date2 = start_date + time_delta1, start_date + time_delta2      ## Valuable date\n",
    "         \n",
    "        df_ref = df_test.loc[val_date1:val_date2]\n",
    "        ## df_max = df_ref.values.max()\n",
    "        if df_ref.empty == False:\n",
    "            df_max = np.max(df_ref)\n",
    "        \n",
    "        ###### Function part: KS_test \n",
    "        KS_time_delta = datetime.timedelta(days = day - days_used_for_prediction) ## time-frame for prediction\n",
    "        KS_val_date1, KS_val_date2 = start_date + KS_time_delta, start_date + time_delta1 ## same as ttest_timedelta +1\n",
    "        KS_df_test = df_test.loc[KS_val_date1 : KS_val_date2]\n",
    "        \n",
    "        if KS_df_test.empty == False:\n",
    "            KS_test_temp = prediction_ks_test(KS_df_test, sample_df)\n",
    "        \n",
    "        deviation_test = abs(df_max - KS_test_temp) ## Absolute difference to real value\n",
    "        \n",
    "        test_results = np.append(test_results, deviation_test)\n",
    "            \n",
    "    test_result = np.mean(test_results)\n",
    "        \n",
    "    print(\"The average deviation of a year: {} K\".format(test_result))\n",
    "\n",
    "    return test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results = prediction_test(df_test, sample_df, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_results))\n",
    "sns.distplot(test_results, kde = False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
